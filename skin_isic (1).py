# -*- coding: utf-8 -*-
"""skin_isic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s7XGnrZJDTPZSCPw8zF9O3buQS_mN0vD

# Import needed libraries
"""

from google.colab import drive
drive.mount('/content/drive')

# import system libs
import os
import time
import shutil
import itertools

# import data handling tools
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
import glob as gb

# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization, Rescaling, RandomZoom, RandomFlip, RandomRotation
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras import regularizers
from tensorflow.keras.metrics import categorical_crossentropy


# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

print ('modules loaded')

"""# Read data"""

train_dir = '/content/drive/MyDrive/Skin_Dieases/Dataset/skintrain'
test_dir = '/content/drive/MyDrive/Skin_Dieases/Dataset/skintest'
valid_dir = '/content/drive/MyDrive/Skin_Dieases/Dataset/skinvalid'
IMG_SIZE = (299, 299) # resolution
BATCH_SIZE = 128
# Create TensorFlow datasets
train_dataset = image_dataset_from_directory(
    train_dir,
    shuffle=True,
    labels='inferred',
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    color_mode='rgb',
    seed=42
)

test_dataset = image_dataset_from_directory(
    test_dir,
    shuffle=True,
    labels='inferred',
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    color_mode='rgb',
    seed=42
)

valid_dataset = image_dataset_from_directory(
    valid_dir,
    shuffle=True,
    labels='inferred',
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    color_mode='rgb',
    seed=42
)

# Initialize lists to hold images and labels
train_images = []
train_labels = []

# Iterate over the batches of the train_dataset
for images, labels in train_dataset:
    for image in images:
        train_images.append(image.numpy())
    for label in labels:
        train_labels.append(label.numpy())

# Convert lists to numpy arrays if needed
train_images = np.array(train_images)
train_labels = np.array(train_labels)

print(train_images.shape)
print(train_labels.shape)

# Initialize lists to hold images and labels
valid_images = []
valid_labels = []

# Iterate over the batches of the valid_dataset
for images, labels in valid_dataset:
    for image in images:
        valid_images.append(image.numpy())
    for label in labels:
        valid_labels.append(label.numpy())

# Convert lists to numpy arrays if needed
valid_images = np.array(valid_images)
valid_labels = np.array(valid_labels)

class_names = train_dataset.class_names
print("Class Names:", class_names)

classes = {0: ('Benign'),
           1: ('Melanoma')}

!pip install keras-utils

normalization_layer = Rescaling(1./255)
train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))

# plt.figure(figsize=(10,10))
# for i in range(25):
#     plt.subplot(5,5,i+1)
#     plt.xticks([])
#     plt.yticks([])
#     plt.grid(False)
#     plt.imshow(train_dataset[i])
#     # The CIFAR labels happen to be arrays,
#     # which is why you need the extra index
#     plt.xlabel(class_names[train_labels[i][0]])
# plt.show()

"""# Is Dataset Balanced?"""

sns.set_theme(style="whitegrid")
Data_imbalance = []
for folder in os.listdir(train_dir):
    files = gb.glob(pathname=str(train_dir + "/" + folder +"/*.*"))
    Data_imbalance.append(len(files))
plt.figure(figsize=(13,7))
sns.barplot(x=["Benign", "Melanoma"], y=Data_imbalance, palette="rocket")
plt.show()

total = 0
for i in range(0,len(Data_imbalance)) :
    total +=Data_imbalance[i]

weight_for_0 = (1 / Data_imbalance[0]) * (total / 2.0)
weight_for_1 = (1 / Data_imbalance[1]) * (total / 2.0)
class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

"""So, dataset is balanced.

# Preprocessing
"""

def preprocess_images(image, label):
    return image, tf.one_hot(label, depth=2)

train_dataset = train_dataset.map(preprocess_images)
valid_dataset = valid_dataset.map(preprocess_images)

"""# Model Structure

**Create ReduceLROnPlateau to learning rate reduction**
"""

from keras.callbacks import ReduceLROnPlateau

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy'
                                            , patience = 2
                                            , verbose=1
                                            ,factor=0.5
                                            , min_lr=0.00001)

model = keras.models.Sequential()

# Create Model Structure
model.add(keras.layers.Input(shape=[299, 299, 3]))
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())

model.add(keras.layers.Flatten())
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=256, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=128, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=64, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.L1L2()))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=2, activation='softmax', kernel_initializer='glorot_uniform', name='classifier'))
model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

model.summary()

"""**Training model**"""

history = model.fit(train_dataset,
                    epochs=25 ,
                    batch_size=BATCH_SIZE,
                    validation_data=(valid_dataset) ,
                    callbacks=[learning_rate_reduction])

"""**Show training history**"""

def plot_training(hist):
    tr_acc = hist.history['accuracy']
    tr_loss = hist.history['loss']
    val_acc = hist.history['val_accuracy']
    val_loss = hist.history['val_loss']
    index_loss = np.argmin(val_loss)
    val_lowest = val_loss[index_loss]
    index_acc = np.argmax(val_acc)
    acc_highest = val_acc[index_acc]

    plt.figure(figsize= (20, 8))
    plt.style.use('fivethirtyeight')
    Epochs = [i+1 for i in range(len(tr_acc))]
    loss_label = f'best epoch= {str(index_loss + 1)}'
    acc_label = f'best epoch= {str(index_acc + 1)}'

    plt.subplot(1, 2, 1)
    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout
    plt.show()

plot_training(history)

test_dataset = test_dataset.map(preprocess_images)

"""**CNN model evaluation**"""

train_score = model.evaluate(train_dataset, verbose= 1)
test_score = model.evaluate(test_dataset, verbose= 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

"""**Saving the model**"""

model.save('/content/drive/MyDrive/Skin HAM10000/output/my_model_isic.h5')

"""Loading CNN model for RF classification"""

from tensorflow import keras
from keras.models import load_model

cnn_model = load_model('/content/drive/MyDrive/Skin HAM10000/output/my_model_isic.h5')

import joblib

joblib.dump(cnn_model, '/content/drive/MyDrive/Skin HAM10000/output/cnn_model_skin_isic.joblib')

from sklearn.ensemble import RandomForestClassifier

loaded_cnn_model = joblib.load('/content/drive/MyDrive/Skin HAM10000/output/cnn_model_skin_isic.joblib')

X_train_features = loaded_cnn_model.predict(train_dataset)
X_test_features = loaded_cnn_model.predict(test_dataset)

X = X_train_features[:, 1:]  # Features: all rows, all columns except the first
y = X_train_features[:, 0]   # Labels: all rows, only the first column

import numpy as np

# Convert probabilities to binary class labels based on a threshold
threshold = 0.5
y_class = np.where(y > threshold, 1, 0)

# Now y_class contains discrete class labels (0 or 1), suitable for classification

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)
rf_classifier.fit(X, y_class)

X1 = X_test_features[:, 1:]  # Features: all rows, all columns except the first
y1 = X_test_features[:, 0]   # Labels: all rows, only the first column

import numpy as np

# Convert probabilities to binary class labels based on a threshold
threshold = 0.5
y1_class = np.where(y1 > threshold, 1, 0)

# Now y_class contains discrete class labels (0 or 1), suitable for classification

# Make predictions using the trained Random Forest classifier
y_pred = rf_classifier.predict(X1)

y_pred.shape

"""# Model Evaluation"""

y_true = np.array(y1_class)

# y_true = np.argmax(y_true)

print(y_pred.shape, y_true.shape)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

"""**Create classes labels**"""

classes_labels = []
for key in classes.keys():
    classes_labels.append(key)

print(classes_labels)

class_names

"""**Confussion Matrix**"""

# Confusion matrix
cm = cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize= (10, 10))
plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation= 45)
plt.yticks(tick_marks, classes)


thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')

plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

plt.show()

from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import cohen_kappa_score

mcc = matthews_corrcoef(y_true, y_pred)
kappa = cohen_kappa_score(y_true, y_pred)

print("Matthews Correlation Coefficient:", mcc)
print("Cohen's Kappa:", kappa)

"""# **ROC**"""

from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize
y_true_binarized = label_binarize(y_true, classes=classes_labels)
y_pred_binarized = label_binarize(y_pred, classes=classes_labels)
print(y_true_binarized.shape)
print(y_pred_binarized.shape)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming y_true_binarized and y_pred_binarized are numpy arrays with correct shapes

# Initialize dictionaries to store fpr, tpr, and roc_auc for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

# Iterate over each class
for i in range(len(class_names)):
    # Calculate ROC curve and AUC for the current class
    fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_binarized[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
plt.figure(figsize=(8, 8))
colors = plt.cm.rainbow(np.linspace(0, 1, len(class_names)))  # Generate colors for each class
for i, color in zip(range(len(class_names)), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='{0} (AUC = {1:0.2f})'.format(classes[class_names[i]][0].upper(), roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Skin Disease Prediction')
plt.legend(loc="lower right")
plt.show()

"""# **Grad CAM Visualization**"""

pip install tf-keras-vis

import matplotlib.cm as cm
cnn_model_for_gradcam=load_model('/content/drive/MyDrive/Skin HAM10000/output/my_model_isic.h5')

last_conv_layer_name = "conv2d_41"
last_conv_layer_model = Model(inputs=cnn_model_for_gradcam.input, outputs=cnn_model_for_gradcam.get_layer(last_conv_layer_name).output)

import os
import pandas as pd

# Define the directory containing class folders
test_dataset_dir = '/content/drive/MyDrive/Skin_Dieases/Dataset/skintest'  # Path to the test directory containing class folders

# Initialize an empty list to store unique images
unique_images = []

# Iterate over each class folder in the test directory
for class_name in os.listdir(test_dataset_dir):
    class_dir = os.path.join(test_dataset_dir, class_name)
    if os.path.isdir(class_dir):
        # Get the first image from each class folder
        image_files = os.listdir(class_dir)
        if image_files:  # Check if the folder contains images
            image_id = image_files[0].split('.')[0]  # Extract image id from the filename
            unique_images.append({'dx': class_name, 'image_id': image_id})

# Create a DataFrame from the list of unique images
unique_images_df = pd.DataFrame(unique_images)

# Display the DataFrame
print(unique_images_df)

def get_image_prediction(image_id):
  img_path = "/content/drive/MyDrive/Skin_Dieases/Dataset/skintest/Benign/"+ image_id + ".jpg"
  img = load_img(img_path, target_size=(299,299))
  img_array = img_to_array(img)
  img_array_expanded = np.expand_dims(img_array, axis=0)
  img_features = cnn_model_for_gradcam.predict(img_array_expanded)
  img_features_flat = img_features.reshape(img_features.shape[0], -1)
  predicted_output = cnn_model_for_gradcam.predict(img_features_flat)
  class_predicted = np.argmax(predicted_output, axis=1)[0]
  return img_array, class_predicted

def model_modifier(m):
    m.layers[-1].activation = tf.keras.activations.linear
    return m

def score_function(output):
    return output[0]

from tf_keras_vis.gradcam import Gradcam
gradcam = Gradcam(cnn_model_for_gradcam, model_modifier, clone=False)

from PIL import Image

def get_heatmap(dx, image_id):
    img_array, class_predicted = get_image_prediction(image_id)
    class_abbr_predicted = classes[class_predicted][0]

    print("img_array shape:", img_array.shape)  # Debugging print statement

    heatmap = gradcam(score_function,
                      img_array,
                      penultimate_layer=-1)  # the last convolutional layer

    print("heatmap shape:", heatmap.shape)  # Debugging print statement

    # Normalize and map the heatmap to RGB
    heatmap_normalized = np.uint8(299 * heatmap)

    # Apply a colormap (like jet) to the heatmap
    heatmap_colored = np.uint8(cm.jet(heatmap_normalized)[..., :3] * 299)

    # Remove the singleton dimensions (if your heatmap has extra dimensions)
    heatmap_colored = np.squeeze(heatmap_colored)

    # Ensure the heatmap is of the correct shape (height, width, channels)
    if heatmap_colored.ndim != 3:
        raise ValueError("Heatmap is not a 3D array of shape (height, width, channels)")

    # Convert the colored heatmap to an Image object
    heatmap_image = Image.fromarray(heatmap_colored)

    # Resize the heatmap to match the original image size
    heatmap_resized = heatmap_image.resize(img_array.shape[:2], Image.ANTIALIAS)

    # Convert the resized heatmap to a numpy array
    heatmap_resized = np.array(heatmap_resized)

    # Superimpose the heatmap onto the original image
    superimposed_img = heatmap_resized * 0.4 + img_array

    original_pil = Image.fromarray(img_array.astype('uint8'))
    superimposed_pil = Image.fromarray(superimposed_img.astype('uint8'))

    # Create a new figure for plotting
    plt.figure(figsize=(12, 6))

    # Plot the original image
    plt.subplot(1, 2, 1)
    plt.imshow(original_pil)
    plt.title(f'Original Image\nTrue class: {dx}')
    plt.axis('off')

    # Plot the heatmap image
    plt.subplot(1, 2, 2)
    plt.imshow(superimposed_pil)
    plt.title(f'GRAD-CAM Class Activation\nPredicted class: {class_abbr_predicted}')
    plt.axis('off')

    # Adjust layout and display the plot
    plt.tight_layout()
    plt.show()

from PIL import Image

# Assuming you have defined the get_heatmap function here

for index, image_info in unique_images_df.iterrows():
    dx = image_info['dx']
    image_id = image_info['image_id']
    get_heatmap(dx, image_id)